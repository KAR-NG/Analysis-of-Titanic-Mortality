---
title: "titanic"
author: "Kar"
date: "2021"
output: 
  github_document:
    toc: true
    toc_depth: 3
always_allow_html: yes
---

## R PACKAGES

```{r, warning=FALSE, message=FALSE}
library(e1071)
library(caret)
library(doSNOW)  # allow training in parallel 
library(ipred)
library(xgboost)
library(skimr)
library(tidyverse)

```

## SUMMARY

This is a side project for learning purposes and practicing effective codes and workflows that I learnt from other data scientists. 

## DATA IMPORT

```{r}
train.data <- read.csv("train.csv")
train.data

```

* PassengerId is worse than useless - will be removed.
* Name is too complicated - will be removed.
* Ticket is too complicated - will be removed.
* Cabin doesn't have a lot of data in it - will be removed.

## DATA WRANGLING

### Changing Data type

This section changes appropriate variables into factors to ease exploration, especially to help with the detection of missing values in the dataset. This method helps to identify missing values for categorical variables.

```{r}
train <- train.data %>% 
  mutate(Survived = as.factor(Survived),
         Pclass = as.factor(Pclass),
         Sex = as.factor(Sex),
         Cabin = as.factor(Cabin),
         Embarked = as.factor(Embarked))

```

Using the function "summary".

```{r}
summary(train)

```


Identify that:

* There are 177 missing values in **Age**, annotated with NA'.

* There are 687 missing values in **Cabin**.

* There are 2 missing values for **Embarked**.


### Missing-value imputation - Embarked

**Embarked**
 
There are two missing values in the "Embarked" formula. I will use the mode value "s" to impute into these 2 value-missing blanks. The reason being that (1) There are only 2 missing values, (2) the level "S" is overwhelming higher than the other two values. 

```{r}
train$Embarked[train$Embarked == ""] <- "S"

```

**Age + Tracking column**

Use sophisticated techniques from caret to help decide ages. I will create a tracking column  

```{r}

train$MissingAge <- ifelse(is.na(train$Age), "Yes", "No")

```

*Cabin*

Cabin will not be used for this analysis as there are too many missing values.


### Feature Engineering

**Adding variable - FamilySize**

This section will try to create new relevant variables, if there is any. Although I am not sure how effective this new feature will be at prediction in this stage, but I will try it. 

```{r}
train$FamilitySize <- 1 + train$SibSp + train$Parch 

```



### Remove variables

Remove PassengerID, Name, Ticket and Cabin. They are irrelevant and redundant to this analysis.

```{r}
train <- train %>% select(-PassengerId, -Name, -Ticket, -Cabin)

```

### Data transformation

First, transform all features to dummy variables. 

```{r}
dummy.vars <- dummyVars(~., data = train[, -1])   # You should exclude respond variable when imputing predictors - reason - you won't have response variable for your new data. 
train.dummy <- dummy.vars %>% predict(train[, -1])
train.dummy <- as_tibble(train.dummy)
train.dummy

```


Now, everything is in dummy variable. 


### Missing-value imputation - Age

Using "bag decision tree imputation", it is a computational expensive method but with high predictive power. It only suitable for small dataset to avoid long waiting time. Caret create imputation model for every column and predict the associated missing values. 

```{r}
pre.process <- preProcess(train.dummy, method = "bagImpute")
imputed.data <- pre.process %>% predict(train.dummy)
imputed.data      # Now missing values in Age has been filled up. 

```

Now, I feed this imputate age column back to the original dataset, and it will become one of the feature that we used to train the XP boost model and the model will figure out in particular age ranges make a difference in predictiving whether you survived or die 


```{r}
# Overwrite the Age of the original dataset

train$Age <- imputed.data$Age      

```

Now, the data is complete. 



## MODELING
 
### Create Data Partition

```{r}
set.seed(54321)

train70 <- createDataPartition(train$Survived,     # Caret calculates relative proportion for y during split 
                               times = 1,          # I can ask for split more than 1 time, can be 5 or 25 times
                               p = 0.7,
                               list = F)            # So you will preserve and left only Row number.

titanic.train <- train[train70, ]
titanic.test <- train[-train70, ]


```

Set up caret to perform 10-fold cross validation repeated 3 times and to use a grid search for optimal model hyperparameter value. CV estimate how well our model will function in production once it is in the real world on brand-new data. It is an estimation process. I can run this random process multiple times, it is more liely to get a better estimate. 

```{r}

train.control <- trainControl(method = "repeatedcv",
                              number = 10,
                              repeats = 3, 
                              search = "grid")  # enable it to do a grid search, go through a collection of parameters and find which ones are optimal, we will show you next section.


```

Leverage a grid search of hyperparameters for xgboost. See following presentation for more information: https://www.slideshare.net/odsc/owen-zhangopen-sourcetoolsanddscompetitions1 

```{r}
tune.grid <- expand.grid(eta = c(0.05, 0.075, 0.1),
                         nrounds = c(50, 75, 100),
                         max_depth = 6:8, 
                         min_child_weight = c(2.0, 2.25, 2.5),
                         colsample_bytree = c(0.3, 0.4, 0.5),
                         gamma = 0,
                         subsample = 1)

# This expand grid function will essentially creates all the permutations of all of these things, in permutations data frame. It is essentially one unique row for all the combinations

tune.grid 

```

There are 243 distinct combinations, so now I am asking caret to run 10-fold cross-validation, repeat 3 times, for each one of these 243 potential values. So... 10 * 3 * 243. 

It is kind of exhaustive grid search that might takes day. The more complicated the model, the more knobs and dials, the more likely is you have to do stuff like this. 

Use the doSNOW package to enable caret to train in *parallel* (Do things at the same time). While there are many package options in the space, doSNOW has the advantage of working on both Windows and Mac OS X.

Create a socket cluster using 10 processes

**NOTE** - Tune this number based on the number of cores/threads available on your machine!!!

```{r}
CI <- makeCluster(3, type = "SOCK")    # Use 10 if you have great computer, small laptop use 3.
  
```

Register cluster so that caret will know to train in parellel. 

```{r}
registerDoSNOW(CI)

```

Train the xgboost model using 10-fold CV repeated 3 yimrd snf s hypermeter grid search to train the optimal model.

```{r}
caret.cv <- train(Survived ~., 
                  data = titanic.train,
                  method = "xgbTree",
                  tuneGrid = tune.grid,
                  trControl = train.control)

stopCluster(CI)

```

Examine caret's processing result

```{r}
caret.cv

```

The bottom says which tuning value is the best. Nice one.

```{r}
preds <- caret.cv %>% predict(titanic.test)

```

Use caret confusionMatrix()vfunction to estimate the effectiveness of this model on unseen, new data


```{r}
confusionMatrix(preds, titanic.test$Survived)

```
Sensitivity: Left side of the table
Specificity: Right side of the table

Sensitivity and specificity important to tell how you feature engineering is. 

* You are right now very good at sensitivity, which means has the right feature to classify who die (0).

* Now you rework features engineering to improve specificity, to rework variables that help to identify who survive (1), because you have room for improvement in specificity. 







## REFERENCE

https://www.kaggle.com/c/titanic/data?select=gender_submission.csv

https://www.youtube.com/watch?v=z8PRU46I3NY

